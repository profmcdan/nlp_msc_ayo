{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from tensorflow.python.keras import backend as K\n",
    "from tensorflow.python.keras.layers import Layer\n",
    "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models import KeyedVectors\n",
    "import gensim\n",
    "import numpy as np\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_word_list(text):\n",
    "    # Pre process and convert texts to a list of words\n",
    "    text = str(text)\n",
    "    text = text.lower()\n",
    "\n",
    "    # Clean the text\n",
    "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"cannot \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\",\", \" \", text)\n",
    "    text = re.sub(r\"\\.\", \" \", text)\n",
    "    text = re.sub(r\"!\", \" ! \", text)\n",
    "    text = re.sub(r\"\\/\", \" \", text)\n",
    "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
    "    text = re.sub(r\"\\+\", \" + \", text)\n",
    "    text = re.sub(r\"\\-\", \" - \", text)\n",
    "    text = re.sub(r\"\\=\", \" = \", text)\n",
    "    text = re.sub(r\"'\", \" \", text)\n",
    "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
    "    text = re.sub(r\":\", \" : \", text)\n",
    "    text = re.sub(r\" e g \", \" eg \", text)\n",
    "    text = re.sub(r\" b g \", \" bg \", text)\n",
    "    text = re.sub(r\" u s \", \" american \", text)\n",
    "    text = re.sub(r\"\\0s\", \"0\", text)\n",
    "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
    "    text = re.sub(r\"e - mail\", \"email\", text)\n",
    "    text = re.sub(r\"j k\", \"jk\", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "\n",
    "    text = text.split()\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_w2v_embeddings(df, embedding_dim=300, empty_w2v=False):\n",
    "    vocabs = {}\n",
    "    vocabs_cnt = 0\n",
    "\n",
    "    vocabs_not_w2v = {}\n",
    "    vocabs_not_w2v_cnt = 0\n",
    "\n",
    "    # Stopwords\n",
    "    stops = set(stopwords.words('english'))\n",
    "\n",
    "    # Load word2vec\n",
    "    print(\"Loading word2vec model(it may takes 2-3 mins) ...\")\n",
    "\n",
    "    if empty_w2v:\n",
    "        word2vec = EmptyWord2Vec\n",
    "    else:\n",
    "        word2vec = KeyedVectors.load_word2vec_format(\"data/GoogleNews-vectors-negative300.bin.gz\", binary=True)\n",
    "        # word2vec = gensim.models.word2vec.Word2Vec.load(\"./data/Quora-Question-Pairs.w2v\").wv\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        # Print the number of embedded sentences.\n",
    "        if index != 0 and index % 1000 == 0:\n",
    "            print(\"{:,} sentences embedded.\".format(index), flush=True)\n",
    "\n",
    "        # Iterate through the text of both questions of the row\n",
    "        for question in ['question1', 'question2']:\n",
    "\n",
    "            q2n = []  # q2n -> question numbers representation\n",
    "            for word in text_to_word_list(row[question]):\n",
    "                # Check for unwanted words\n",
    "                if word in stops:\n",
    "                    continue\n",
    "\n",
    "                # If a word is missing from word2vec model.\n",
    "                if word not in word2vec.vocab:\n",
    "                    if word not in vocabs_not_w2v:\n",
    "                        vocabs_not_w2v_cnt += 1\n",
    "                        vocabs_not_w2v[word] = 1\n",
    "\n",
    "                # If you have never seen a word, append it to vocab dictionary.\n",
    "                if word not in vocabs:\n",
    "                    vocabs_cnt += 1\n",
    "                    vocabs[word] = vocabs_cnt\n",
    "                    q2n.append(vocabs_cnt)\n",
    "                else:\n",
    "                    q2n.append(vocabs[word])\n",
    "\n",
    "            # Append question as number representation\n",
    "            df.at[index, question + '_n'] = q2n\n",
    "\n",
    "    embeddings = 1 * np.random.randn(len(vocabs) + 1, embedding_dim)  # This will be the embedding matrix\n",
    "    embeddings[0] = 0  # So that the padding will be ignored\n",
    "\n",
    "    # Build the embedding matrix\n",
    "    for word, index in vocabs.items():\n",
    "        if word in word2vec.vocab:\n",
    "            embeddings[index] = word2vec.word_vec(word)\n",
    "    del word2vec\n",
    "\n",
    "    return df, embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_and_zero_padding(df, max_seq_length):\n",
    "    # Split to dicts\n",
    "    X = {'left': df['question1_n'], 'right': df['question2_n']}\n",
    "\n",
    "    # Zero padding\n",
    "    for dataset, side in itertools.product([X], ['left', 'right']):\n",
    "        dataset[side] = pad_sequences(dataset[side], padding='pre', truncating='post', maxlen=max_seq_length)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ManDist(Layer):\n",
    "    \"\"\"\n",
    "    Keras Custom Layer that calculates Manhattan Distance.\n",
    "    \"\"\"\n",
    "\n",
    "    # initialize the layer, No need to include inputs parameter!\n",
    "    def __init__(self, **kwargs):\n",
    "        self.result = None\n",
    "        super(ManDist, self).__init__(**kwargs)\n",
    "\n",
    "    # input_shape will automatic collect input shapes to build layer\n",
    "    def build(self, input_shape):\n",
    "        super(ManDist, self).build(input_shape)\n",
    "\n",
    "    # This is where the layer's logic lives.\n",
    "    def call(self, x, **kwargs):\n",
    "        self.result = K.exp(-K.sum(K.abs(x[0] - x[1]), axis=1, keepdims=True))\n",
    "        return self.result\n",
    "\n",
    "    # return output shape\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return K.int_shape(self.result)\n",
    "\n",
    "\n",
    "class EmptyWord2Vec:\n",
    "    \"\"\"\n",
    "    Just for test use.\n",
    "    \"\"\"\n",
    "    vocab = {}\n",
    "    word_vec = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading word2vec model(it may takes 2-3 mins) ...\n"
     ]
    }
   ],
   "source": [
    "# File paths\n",
    "TEST_CSV = 'data/test_20.csv'\n",
    "\n",
    "# Load training set\n",
    "test_df = pd.read_csv(TEST_CSV, low_memory=False)\n",
    "for q in ['question1', 'question2']:\n",
    "    test_df[q + '_n'] = test_df[q]\n",
    "\n",
    "# Make word2vec embeddings\n",
    "embedding_dim = 300\n",
    "max_seq_length = 20\n",
    "test_df, embeddings = make_w2v_embeddings(test_df, embedding_dim=embedding_dim, empty_w2v=False)\n",
    "\n",
    "# Split to dicts and append zero padding.\n",
    "X_test = split_and_zero_padding(test_df, max_seq_length)\n",
    "\n",
    "# Make sure everything is ok\n",
    "assert X_test['left'].shape == X_test['right'].shape\n",
    "\n",
    "# --"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 20)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 20)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "sequential (Sequential)         (None, 50)           25827000    input_1[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "man_dist (ManDist)              (None, 1)            0           sequential[1][0]                 \n",
      "                                                                 sequential[2][0]                 \n",
      "==================================================================================================\n",
      "Total params: 25,827,000\n",
      "Trainable params: 70,200\n",
      "Non-trainable params: 25,756,800\n",
      "__________________________________________________________________________________________________\n",
      "[[1.1901250e-02]\n",
      " [4.8346433e-01]\n",
      " [1.9379978e-01]\n",
      " [2.3884839e-01]\n",
      " [4.6589670e-01]\n",
      " [1.0573619e-01]\n",
      " [9.9619865e-02]\n",
      " [1.9578168e-01]\n",
      " [4.8773482e-01]\n",
      " [2.1540362e-04]\n",
      " [2.4549933e-01]\n",
      " [6.5477109e-03]\n",
      " [3.2362375e-02]\n",
      " [1.7520167e-01]\n",
      " [2.3001928e-02]\n",
      " [2.2193159e-01]\n",
      " [3.6364457e-01]\n",
      " [2.5181699e-01]\n",
      " [4.5873296e-01]\n",
      " [2.4557702e-01]]\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.load_model('SiameseLSTM.h5', custom_objects={'ManDist': ManDist})\n",
    "model.summary()\n",
    "\n",
    "prediction = model.predict([X_test['left'], X_test['right']])\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
