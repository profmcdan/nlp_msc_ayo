{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-02-24 16:25:07,610 : INFO : read 1000 sentences\n",
      "2019-02-24 16:25:07,796 : INFO : read 2000 sentences\n",
      "2019-02-24 16:25:07,985 : INFO : read 3000 sentences\n",
      "2019-02-24 16:25:08,170 : INFO : read 4000 sentences\n",
      "2019-02-24 16:25:08,359 : INFO : read 5000 sentences\n",
      "2019-02-24 16:25:08,548 : INFO : read 6000 sentences\n",
      "2019-02-24 16:25:08,731 : INFO : read 7000 sentences\n",
      "2019-02-24 16:25:08,913 : INFO : read 8000 sentences\n",
      "2019-02-24 16:25:09,101 : INFO : read 9000 sentences\n",
      "2019-02-24 16:25:09,324 : INFO : read 10000 sentences\n",
      "2019-02-24 16:25:09,513 : INFO : read 11000 sentences\n",
      "2019-02-24 16:25:09,699 : INFO : read 12000 sentences\n",
      "2019-02-24 16:25:09,890 : INFO : read 13000 sentences\n",
      "2019-02-24 16:25:10,071 : INFO : read 14000 sentences\n",
      "2019-02-24 16:25:10,255 : INFO : read 15000 sentences\n",
      "2019-02-24 16:25:10,436 : INFO : read 16000 sentences\n",
      "2019-02-24 16:25:10,620 : INFO : read 17000 sentences\n",
      "2019-02-24 16:25:10,804 : INFO : read 18000 sentences\n",
      "2019-02-24 16:25:10,985 : INFO : read 19000 sentences\n",
      "2019-02-24 16:25:11,170 : INFO : read 20000 sentences\n",
      "2019-02-24 16:25:11,353 : INFO : read 21000 sentences\n",
      "2019-02-24 16:25:11,550 : INFO : read 22000 sentences\n",
      "2019-02-24 16:25:11,730 : INFO : read 23000 sentences\n",
      "2019-02-24 16:25:11,910 : INFO : read 24000 sentences\n",
      "2019-02-24 16:25:12,094 : INFO : read 25000 sentences\n",
      "2019-02-24 16:25:12,282 : INFO : read 26000 sentences\n",
      "2019-02-24 16:25:12,468 : INFO : read 27000 sentences\n",
      "2019-02-24 16:25:12,658 : INFO : read 28000 sentences\n",
      "2019-02-24 16:25:12,845 : INFO : read 29000 sentences\n",
      "2019-02-24 16:25:13,045 : INFO : read 30000 sentences\n",
      "2019-02-24 16:25:13,233 : INFO : read 31000 sentences\n",
      "2019-02-24 16:25:13,433 : INFO : read 32000 sentences\n",
      "2019-02-24 16:25:13,624 : INFO : read 33000 sentences\n",
      "2019-02-24 16:25:13,810 : INFO : read 34000 sentences\n",
      "2019-02-24 16:25:13,996 : INFO : read 35000 sentences\n",
      "2019-02-24 16:25:14,175 : INFO : read 36000 sentences\n",
      "2019-02-24 16:25:14,358 : INFO : read 37000 sentences\n",
      "2019-02-24 16:25:14,550 : INFO : read 38000 sentences\n",
      "2019-02-24 16:25:14,733 : INFO : read 39000 sentences\n",
      "2019-02-24 16:25:14,922 : INFO : read 40000 sentences\n",
      "2019-02-24 16:25:15,109 : INFO : read 41000 sentences\n",
      "2019-02-24 16:25:15,313 : INFO : read 42000 sentences\n",
      "2019-02-24 16:25:15,503 : INFO : read 43000 sentences\n",
      "2019-02-24 16:25:15,683 : INFO : read 44000 sentences\n",
      "2019-02-24 16:25:15,868 : INFO : read 45000 sentences\n",
      "2019-02-24 16:25:16,055 : INFO : read 46000 sentences\n",
      "2019-02-24 16:25:16,269 : INFO : read 47000 sentences\n",
      "2019-02-24 16:25:16,451 : INFO : read 48000 sentences\n",
      "2019-02-24 16:25:16,641 : INFO : read 49000 sentences\n",
      "2019-02-24 16:25:16,826 : INFO : read 50000 sentences\n",
      "2019-02-24 16:25:17,010 : INFO : read 51000 sentences\n",
      "2019-02-24 16:25:17,202 : INFO : read 52000 sentences\n",
      "2019-02-24 16:25:17,393 : INFO : read 53000 sentences\n",
      "2019-02-24 16:25:17,578 : INFO : read 54000 sentences\n",
      "2019-02-24 16:25:17,763 : INFO : read 55000 sentences\n",
      "2019-02-24 16:25:18,010 : INFO : read 56000 sentences\n",
      "2019-02-24 16:25:18,190 : INFO : read 57000 sentences\n",
      "2019-02-24 16:25:18,375 : INFO : read 58000 sentences\n",
      "2019-02-24 16:25:18,567 : INFO : read 59000 sentences\n",
      "2019-02-24 16:25:18,750 : INFO : read 60000 sentences\n",
      "2019-02-24 16:25:18,932 : INFO : read 61000 sentences\n",
      "2019-02-24 16:25:19,111 : INFO : read 62000 sentences\n",
      "2019-02-24 16:25:19,293 : INFO : read 63000 sentences\n",
      "2019-02-24 16:25:19,476 : INFO : read 64000 sentences\n",
      "2019-02-24 16:25:19,662 : INFO : read 65000 sentences\n",
      "2019-02-24 16:25:19,850 : INFO : read 66000 sentences\n",
      "2019-02-24 16:25:20,033 : INFO : read 67000 sentences\n",
      "2019-02-24 16:25:20,220 : INFO : read 68000 sentences\n",
      "2019-02-24 16:25:20,401 : INFO : read 69000 sentences\n",
      "2019-02-24 16:25:20,592 : INFO : read 70000 sentences\n",
      "2019-02-24 16:25:20,775 : INFO : read 71000 sentences\n",
      "2019-02-24 16:25:20,959 : INFO : read 72000 sentences\n",
      "2019-02-24 16:25:21,141 : INFO : read 73000 sentences\n",
      "2019-02-24 16:25:21,324 : INFO : read 74000 sentences\n",
      "2019-02-24 16:25:21,513 : INFO : read 75000 sentences\n",
      "2019-02-24 16:25:21,695 : INFO : read 76000 sentences\n",
      "2019-02-24 16:25:21,876 : INFO : read 77000 sentences\n",
      "2019-02-24 16:25:22,063 : INFO : read 78000 sentences\n",
      "2019-02-24 16:25:22,246 : INFO : read 79000 sentences\n",
      "2019-02-24 16:25:22,432 : INFO : read 80000 sentences\n",
      "2019-02-24 16:25:22,619 : INFO : read 81000 sentences\n",
      "2019-02-24 16:25:22,807 : INFO : read 82000 sentences\n",
      "2019-02-24 16:25:22,990 : INFO : read 83000 sentences\n",
      "2019-02-24 16:25:23,171 : INFO : read 84000 sentences\n",
      "2019-02-24 16:25:23,356 : INFO : read 85000 sentences\n",
      "2019-02-24 16:25:23,555 : INFO : read 86000 sentences\n",
      "2019-02-24 16:25:23,734 : INFO : read 87000 sentences\n",
      "2019-02-24 16:25:23,918 : INFO : read 88000 sentences\n",
      "2019-02-24 16:25:24,110 : INFO : read 89000 sentences\n",
      "2019-02-24 16:25:24,297 : INFO : read 90000 sentences\n",
      "2019-02-24 16:25:24,477 : INFO : read 91000 sentences\n",
      "2019-02-24 16:25:24,672 : INFO : read 92000 sentences\n",
      "2019-02-24 16:25:24,857 : INFO : read 93000 sentences\n",
      "2019-02-24 16:25:25,041 : INFO : read 94000 sentences\n",
      "2019-02-24 16:25:25,229 : INFO : read 95000 sentences\n",
      "2019-02-24 16:25:25,408 : INFO : read 96000 sentences\n",
      "2019-02-24 16:25:25,593 : INFO : read 97000 sentences\n",
      "2019-02-24 16:25:25,778 : INFO : read 98000 sentences\n",
      "2019-02-24 16:25:25,964 : INFO : read 99000 sentences\n",
      "2019-02-24 16:25:26,149 : INFO : read 100000 sentences\n",
      "2019-02-24 16:25:26,340 : INFO : read 101000 sentences\n",
      "2019-02-24 16:25:26,651 : INFO : read 102000 sentences\n",
      "2019-02-24 16:25:26,858 : INFO : read 103000 sentences\n",
      "2019-02-24 16:25:27,067 : INFO : read 104000 sentences\n",
      "2019-02-24 16:25:27,283 : INFO : read 105000 sentences\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "decoding to str: need a bytes-like object, float found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-3fb0fec34149>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m \u001b[0mdocuments\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mextract_questions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m \u001b[0mlogging\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Done reading data file\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-3fb0fec34149>\u001b[0m in \u001b[0;36mextract_questions\u001b[1;34m()\u001b[0m\n\u001b[0;32m     21\u001b[0m                 \u001b[1;32myield\u001b[0m \u001b[0mgensim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msimple_preprocess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'question1'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mrow\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'question2'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m                 \u001b[1;32myield\u001b[0m \u001b[0mgensim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msimple_preprocess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'question2'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gensim\\utils.py\u001b[0m in \u001b[0;36msimple_preprocess\u001b[1;34m(doc, deacc, min_len, max_len)\u001b[0m\n\u001b[0;32m    303\u001b[0m     \"\"\"\n\u001b[0;32m    304\u001b[0m     tokens = [\n\u001b[1;32m--> 305\u001b[1;33m         \u001b[0mtoken\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlower\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdeacc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdeacc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'ignore'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    306\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mmin_len\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[0mmax_len\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mtoken\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'_'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    307\u001b[0m     ]\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gensim\\utils.py\u001b[0m in \u001b[0;36mtokenize\u001b[1;34m(text, lowercase, deacc, encoding, errors, to_lower, lower)\u001b[0m\n\u001b[0;32m    254\u001b[0m     \"\"\"\n\u001b[0;32m    255\u001b[0m     \u001b[0mlowercase\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlowercase\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mto_lower\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mlower\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 256\u001b[1;33m     \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_unicode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    257\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlowercase\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    258\u001b[0m         \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gensim\\utils.py\u001b[0m in \u001b[0;36many2unicode\u001b[1;34m(text, encoding, errors)\u001b[0m\n\u001b[0;32m    357\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0municode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    358\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 359\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0municode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    360\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    361\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: decoding to str: need a bytes-like object, float found"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "import logging\n",
    "import pandas as pd\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "\n",
    "def extract_questions():\n",
    "    \"\"\"\n",
    "    Extract questions for making word2vec model.\n",
    "    \"\"\"\n",
    "    df1 = pd.read_csv(\"data/train.csv\", low_memory=False)\n",
    "    df2 = pd.read_csv(\"data/test.csv\", low_memory=False)\n",
    "\n",
    "    for dataset in [df1, df2]:\n",
    "        for i, row in dataset.iterrows():\n",
    "            if i != 0 and i % 1000 == 0:\n",
    "                logging.info(\"read {0} sentences\".format(i))\n",
    "\n",
    "            if row['question1']:\n",
    "                yield gensim.utils.simple_preprocess(row['question1'])\n",
    "            if row['question2']:\n",
    "                yield gensim.utils.simple_preprocess(row['question2'])\n",
    "\n",
    "\n",
    "documents = list(extract_questions())\n",
    "logging.info(\"Done reading data file\")\n",
    "\n",
    "model = gensim.models.Word2Vec(documents, size=300)\n",
    "model.train(documents, total_examples=len(documents), epochs=10)\n",
    "model.save(\"data/Quora-Question-Pairs.w2v\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from tensorflow.python.keras import backend as K\n",
    "from tensorflow.python.keras.layers import Layer\n",
    "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "import gensim\n",
    "import numpy as np\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_word_list(text):\n",
    "    # Pre process and convert texts to a list of words\n",
    "    text = str(text)\n",
    "    text = text.lower()\n",
    "\n",
    "    # Clean the text\n",
    "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"cannot \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\",\", \" \", text)\n",
    "    text = re.sub(r\"\\.\", \" \", text)\n",
    "    text = re.sub(r\"!\", \" ! \", text)\n",
    "    text = re.sub(r\"\\/\", \" \", text)\n",
    "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
    "    text = re.sub(r\"\\+\", \" + \", text)\n",
    "    text = re.sub(r\"\\-\", \" - \", text)\n",
    "    text = re.sub(r\"\\=\", \" = \", text)\n",
    "    text = re.sub(r\"'\", \" \", text)\n",
    "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
    "    text = re.sub(r\":\", \" : \", text)\n",
    "    text = re.sub(r\" e g \", \" eg \", text)\n",
    "    text = re.sub(r\" b g \", \" bg \", text)\n",
    "    text = re.sub(r\" u s \", \" american \", text)\n",
    "    text = re.sub(r\"\\0s\", \"0\", text)\n",
    "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
    "    text = re.sub(r\"e - mail\", \"email\", text)\n",
    "    text = re.sub(r\"j k\", \"jk\", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "\n",
    "    text = text.split()\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_w2v_embeddings(df, embedding_dim=300, empty_w2v=False):\n",
    "    vocabs = {}\n",
    "    vocabs_cnt = 0\n",
    "\n",
    "    vocabs_not_w2v = {}\n",
    "    vocabs_not_w2v_cnt = 0\n",
    "\n",
    "    # Stopwords\n",
    "    stops = set(stopwords.words('english'))\n",
    "\n",
    "    # Load word2vec\n",
    "    print(\"Loading word2vec model(it may takes 2-3 mins) ...\")\n",
    "\n",
    "    if empty_w2v:\n",
    "        word2vec = EmptyWord2Vec\n",
    "    else:\n",
    "        word2vec = KeyedVectors.load_word2vec_format(\"data/GoogleNews-vectors-negative300.bin.gz\", binary=True)\n",
    "        # word2vec = gensim.models.word2vec.Word2Vec.load(\"./data/Quora-Question-Pairs.w2v\").wv\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        # Print the number of embedded sentences.\n",
    "        if index != 0 and index % 1000 == 0:\n",
    "            print(\"{:,} sentences embedded.\".format(index), flush=True)\n",
    "\n",
    "        # Iterate through the text of both questions of the row\n",
    "        for question in ['question1', 'question2']:\n",
    "\n",
    "            q2n = []  # q2n -> question numbers representation\n",
    "            for word in text_to_word_list(row[question]):\n",
    "                # Check for unwanted words\n",
    "                if word in stops:\n",
    "                    continue\n",
    "\n",
    "                # If a word is missing from word2vec model.\n",
    "                if word not in word2vec.vocab:\n",
    "                    if word not in vocabs_not_w2v:\n",
    "                        vocabs_not_w2v_cnt += 1\n",
    "                        vocabs_not_w2v[word] = 1\n",
    "\n",
    "                # If you have never seen a word, append it to vocab dictionary.\n",
    "                if word not in vocabs:\n",
    "                    vocabs_cnt += 1\n",
    "                    vocabs[word] = vocabs_cnt\n",
    "                    q2n.append(vocabs_cnt)\n",
    "                else:\n",
    "                    q2n.append(vocabs[word])\n",
    "\n",
    "            # Append question as number representation\n",
    "            df.at[index, question + '_n'] = q2n\n",
    "\n",
    "    embeddings = 1 * np.random.randn(len(vocabs) + 1, embedding_dim)  # This will be the embedding matrix\n",
    "    embeddings[0] = 0  # So that the padding will be ignored\n",
    "\n",
    "    # Build the embedding matrix\n",
    "    for word, index in vocabs.items():\n",
    "        if word in word2vec.vocab:\n",
    "            embeddings[index] = word2vec.word_vec(word)\n",
    "    del word2vec\n",
    "\n",
    "    return df, embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_and_zero_padding(df, max_seq_length):\n",
    "    # Split to dicts\n",
    "    X = {'left': df['question1_n'], 'right': df['question2_n']}\n",
    "\n",
    "    # Zero padding\n",
    "    for dataset, side in itertools.product([X], ['left', 'right']):\n",
    "        dataset[side] = pad_sequences(dataset[side], padding='pre', truncating='post', maxlen=max_seq_length)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ManDist(Layer):\n",
    "    \"\"\"\n",
    "    Keras Custom Layer that calculates Manhattan Distance.\n",
    "    \"\"\"\n",
    "\n",
    "    # initialize the layer, No need to include inputs parameter!\n",
    "    def __init__(self, **kwargs):\n",
    "        self.result = None\n",
    "        super(ManDist, self).__init__(**kwargs)\n",
    "\n",
    "    # input_shape will automatic collect input shapes to build layer\n",
    "    def build(self, input_shape):\n",
    "        super(ManDist, self).build(input_shape)\n",
    "\n",
    "    # This is where the layer's logic lives.\n",
    "    def call(self, x, **kwargs):\n",
    "        self.result = K.exp(-K.sum(K.abs(x[0] - x[1]), axis=1, keepdims=True))\n",
    "        return self.result\n",
    "\n",
    "    # return output shape\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return K.int_shape(self.result)\n",
    "\n",
    "\n",
    "class EmptyWord2Vec:\n",
    "    \"\"\"\n",
    "    Just for test use.\n",
    "    \"\"\"\n",
    "    vocab = {}\n",
    "    word_vec = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib\n",
    "\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.python.keras.models import Model, Sequential\n",
    "from tensorflow.python.keras.layers import Input, Embedding, LSTM, GRU, Conv1D, Conv2D, GlobalMaxPool1D, Dense, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-02-24 16:27:46,248 : INFO : loading projection weights from data/GoogleNews-vectors-negative300.bin.gz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading word2vec model(it may takes 2-3 mins) ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-02-24 16:30:41,892 : INFO : loaded (3000000, 300) matrix from data/GoogleNews-vectors-negative300.bin.gz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1,000 sentences embedded.\n",
      "2,000 sentences embedded.\n",
      "3,000 sentences embedded.\n",
      "4,000 sentences embedded.\n",
      "5,000 sentences embedded.\n",
      "6,000 sentences embedded.\n",
      "7,000 sentences embedded.\n",
      "8,000 sentences embedded.\n",
      "9,000 sentences embedded.\n",
      "10,000 sentences embedded.\n",
      "11,000 sentences embedded.\n",
      "12,000 sentences embedded.\n",
      "13,000 sentences embedded.\n",
      "14,000 sentences embedded.\n",
      "15,000 sentences embedded.\n",
      "16,000 sentences embedded.\n",
      "17,000 sentences embedded.\n",
      "18,000 sentences embedded.\n",
      "19,000 sentences embedded.\n",
      "20,000 sentences embedded.\n",
      "21,000 sentences embedded.\n",
      "22,000 sentences embedded.\n",
      "23,000 sentences embedded.\n",
      "24,000 sentences embedded.\n",
      "25,000 sentences embedded.\n",
      "26,000 sentences embedded.\n",
      "27,000 sentences embedded.\n",
      "28,000 sentences embedded.\n",
      "29,000 sentences embedded.\n",
      "30,000 sentences embedded.\n",
      "31,000 sentences embedded.\n",
      "32,000 sentences embedded.\n",
      "33,000 sentences embedded.\n",
      "34,000 sentences embedded.\n",
      "35,000 sentences embedded.\n",
      "36,000 sentences embedded.\n",
      "37,000 sentences embedded.\n",
      "38,000 sentences embedded.\n",
      "39,000 sentences embedded.\n",
      "40,000 sentences embedded.\n",
      "41,000 sentences embedded.\n",
      "42,000 sentences embedded.\n",
      "43,000 sentences embedded.\n",
      "44,000 sentences embedded.\n",
      "45,000 sentences embedded.\n",
      "46,000 sentences embedded.\n",
      "47,000 sentences embedded.\n",
      "48,000 sentences embedded.\n",
      "49,000 sentences embedded.\n",
      "50,000 sentences embedded.\n",
      "51,000 sentences embedded.\n",
      "52,000 sentences embedded.\n",
      "53,000 sentences embedded.\n",
      "54,000 sentences embedded.\n",
      "55,000 sentences embedded.\n",
      "56,000 sentences embedded.\n",
      "57,000 sentences embedded.\n",
      "58,000 sentences embedded.\n",
      "59,000 sentences embedded.\n",
      "60,000 sentences embedded.\n",
      "61,000 sentences embedded.\n",
      "62,000 sentences embedded.\n",
      "63,000 sentences embedded.\n",
      "64,000 sentences embedded.\n",
      "65,000 sentences embedded.\n",
      "66,000 sentences embedded.\n",
      "67,000 sentences embedded.\n",
      "68,000 sentences embedded.\n",
      "69,000 sentences embedded.\n",
      "70,000 sentences embedded.\n",
      "71,000 sentences embedded.\n",
      "72,000 sentences embedded.\n",
      "73,000 sentences embedded.\n",
      "74,000 sentences embedded.\n",
      "75,000 sentences embedded.\n",
      "76,000 sentences embedded.\n",
      "77,000 sentences embedded.\n",
      "78,000 sentences embedded.\n",
      "79,000 sentences embedded.\n",
      "80,000 sentences embedded.\n",
      "81,000 sentences embedded.\n",
      "82,000 sentences embedded.\n",
      "83,000 sentences embedded.\n",
      "84,000 sentences embedded.\n",
      "85,000 sentences embedded.\n",
      "86,000 sentences embedded.\n",
      "87,000 sentences embedded.\n",
      "88,000 sentences embedded.\n",
      "89,000 sentences embedded.\n",
      "90,000 sentences embedded.\n",
      "91,000 sentences embedded.\n",
      "92,000 sentences embedded.\n",
      "93,000 sentences embedded.\n",
      "94,000 sentences embedded.\n",
      "95,000 sentences embedded.\n",
      "96,000 sentences embedded.\n",
      "97,000 sentences embedded.\n",
      "98,000 sentences embedded.\n",
      "99,000 sentences embedded.\n",
      "100,000 sentences embedded.\n",
      "101,000 sentences embedded.\n",
      "102,000 sentences embedded.\n",
      "103,000 sentences embedded.\n",
      "104,000 sentences embedded.\n",
      "105,000 sentences embedded.\n",
      "106,000 sentences embedded.\n",
      "107,000 sentences embedded.\n",
      "108,000 sentences embedded.\n",
      "109,000 sentences embedded.\n",
      "110,000 sentences embedded.\n",
      "111,000 sentences embedded.\n",
      "112,000 sentences embedded.\n",
      "113,000 sentences embedded.\n",
      "114,000 sentences embedded.\n",
      "115,000 sentences embedded.\n",
      "116,000 sentences embedded.\n",
      "117,000 sentences embedded.\n",
      "118,000 sentences embedded.\n",
      "119,000 sentences embedded.\n",
      "120,000 sentences embedded.\n",
      "121,000 sentences embedded.\n",
      "122,000 sentences embedded.\n",
      "123,000 sentences embedded.\n",
      "124,000 sentences embedded.\n",
      "125,000 sentences embedded.\n",
      "126,000 sentences embedded.\n",
      "127,000 sentences embedded.\n",
      "128,000 sentences embedded.\n",
      "129,000 sentences embedded.\n",
      "130,000 sentences embedded.\n",
      "131,000 sentences embedded.\n",
      "132,000 sentences embedded.\n",
      "133,000 sentences embedded.\n",
      "134,000 sentences embedded.\n",
      "135,000 sentences embedded.\n",
      "136,000 sentences embedded.\n",
      "137,000 sentences embedded.\n",
      "138,000 sentences embedded.\n",
      "139,000 sentences embedded.\n",
      "140,000 sentences embedded.\n",
      "141,000 sentences embedded.\n",
      "142,000 sentences embedded.\n",
      "143,000 sentences embedded.\n",
      "144,000 sentences embedded.\n",
      "145,000 sentences embedded.\n",
      "146,000 sentences embedded.\n",
      "147,000 sentences embedded.\n",
      "148,000 sentences embedded.\n",
      "149,000 sentences embedded.\n",
      "150,000 sentences embedded.\n",
      "151,000 sentences embedded.\n",
      "152,000 sentences embedded.\n",
      "153,000 sentences embedded.\n",
      "154,000 sentences embedded.\n",
      "155,000 sentences embedded.\n",
      "156,000 sentences embedded.\n",
      "157,000 sentences embedded.\n",
      "158,000 sentences embedded.\n",
      "159,000 sentences embedded.\n",
      "160,000 sentences embedded.\n",
      "161,000 sentences embedded.\n",
      "162,000 sentences embedded.\n",
      "163,000 sentences embedded.\n",
      "164,000 sentences embedded.\n",
      "165,000 sentences embedded.\n",
      "166,000 sentences embedded.\n",
      "167,000 sentences embedded.\n",
      "168,000 sentences embedded.\n",
      "169,000 sentences embedded.\n",
      "170,000 sentences embedded.\n",
      "171,000 sentences embedded.\n",
      "172,000 sentences embedded.\n",
      "173,000 sentences embedded.\n",
      "174,000 sentences embedded.\n",
      "175,000 sentences embedded.\n",
      "176,000 sentences embedded.\n",
      "177,000 sentences embedded.\n",
      "178,000 sentences embedded.\n",
      "179,000 sentences embedded.\n",
      "180,000 sentences embedded.\n",
      "181,000 sentences embedded.\n",
      "182,000 sentences embedded.\n",
      "183,000 sentences embedded.\n",
      "184,000 sentences embedded.\n",
      "185,000 sentences embedded.\n",
      "186,000 sentences embedded.\n",
      "187,000 sentences embedded.\n",
      "188,000 sentences embedded.\n",
      "189,000 sentences embedded.\n",
      "190,000 sentences embedded.\n",
      "191,000 sentences embedded.\n",
      "192,000 sentences embedded.\n",
      "193,000 sentences embedded.\n",
      "194,000 sentences embedded.\n",
      "195,000 sentences embedded.\n",
      "196,000 sentences embedded.\n",
      "197,000 sentences embedded.\n",
      "198,000 sentences embedded.\n",
      "199,000 sentences embedded.\n",
      "200,000 sentences embedded.\n",
      "201,000 sentences embedded.\n",
      "202,000 sentences embedded.\n",
      "203,000 sentences embedded.\n",
      "204,000 sentences embedded.\n",
      "205,000 sentences embedded.\n",
      "206,000 sentences embedded.\n",
      "207,000 sentences embedded.\n",
      "208,000 sentences embedded.\n",
      "209,000 sentences embedded.\n",
      "210,000 sentences embedded.\n",
      "211,000 sentences embedded.\n",
      "212,000 sentences embedded.\n",
      "213,000 sentences embedded.\n",
      "214,000 sentences embedded.\n",
      "215,000 sentences embedded.\n",
      "216,000 sentences embedded.\n",
      "217,000 sentences embedded.\n",
      "218,000 sentences embedded.\n",
      "219,000 sentences embedded.\n",
      "220,000 sentences embedded.\n",
      "221,000 sentences embedded.\n",
      "222,000 sentences embedded.\n",
      "223,000 sentences embedded.\n",
      "224,000 sentences embedded.\n",
      "225,000 sentences embedded.\n",
      "226,000 sentences embedded.\n",
      "227,000 sentences embedded.\n",
      "228,000 sentences embedded.\n",
      "229,000 sentences embedded.\n",
      "230,000 sentences embedded.\n",
      "231,000 sentences embedded.\n",
      "232,000 sentences embedded.\n",
      "233,000 sentences embedded.\n",
      "234,000 sentences embedded.\n",
      "235,000 sentences embedded.\n",
      "236,000 sentences embedded.\n",
      "237,000 sentences embedded.\n",
      "238,000 sentences embedded.\n",
      "239,000 sentences embedded.\n",
      "240,000 sentences embedded.\n",
      "241,000 sentences embedded.\n",
      "242,000 sentences embedded.\n",
      "243,000 sentences embedded.\n",
      "244,000 sentences embedded.\n",
      "245,000 sentences embedded.\n",
      "246,000 sentences embedded.\n",
      "247,000 sentences embedded.\n",
      "248,000 sentences embedded.\n",
      "249,000 sentences embedded.\n",
      "250,000 sentences embedded.\n",
      "251,000 sentences embedded.\n",
      "252,000 sentences embedded.\n",
      "253,000 sentences embedded.\n",
      "254,000 sentences embedded.\n",
      "255,000 sentences embedded.\n",
      "256,000 sentences embedded.\n",
      "257,000 sentences embedded.\n",
      "258,000 sentences embedded.\n",
      "259,000 sentences embedded.\n",
      "260,000 sentences embedded.\n",
      "261,000 sentences embedded.\n",
      "262,000 sentences embedded.\n",
      "263,000 sentences embedded.\n",
      "264,000 sentences embedded.\n",
      "265,000 sentences embedded.\n",
      "266,000 sentences embedded.\n",
      "267,000 sentences embedded.\n",
      "268,000 sentences embedded.\n",
      "269,000 sentences embedded.\n",
      "270,000 sentences embedded.\n",
      "271,000 sentences embedded.\n",
      "272,000 sentences embedded.\n",
      "273,000 sentences embedded.\n",
      "274,000 sentences embedded.\n",
      "275,000 sentences embedded.\n",
      "276,000 sentences embedded.\n",
      "277,000 sentences embedded.\n",
      "278,000 sentences embedded.\n",
      "279,000 sentences embedded.\n",
      "280,000 sentences embedded.\n",
      "281,000 sentences embedded.\n",
      "282,000 sentences embedded.\n",
      "283,000 sentences embedded.\n",
      "284,000 sentences embedded.\n",
      "285,000 sentences embedded.\n",
      "286,000 sentences embedded.\n",
      "287,000 sentences embedded.\n",
      "288,000 sentences embedded.\n",
      "289,000 sentences embedded.\n",
      "290,000 sentences embedded.\n",
      "291,000 sentences embedded.\n",
      "292,000 sentences embedded.\n",
      "293,000 sentences embedded.\n",
      "294,000 sentences embedded.\n",
      "295,000 sentences embedded.\n",
      "296,000 sentences embedded.\n",
      "297,000 sentences embedded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "298,000 sentences embedded.\n",
      "299,000 sentences embedded.\n",
      "300,000 sentences embedded.\n",
      "301,000 sentences embedded.\n",
      "302,000 sentences embedded.\n",
      "303,000 sentences embedded.\n",
      "304,000 sentences embedded.\n",
      "305,000 sentences embedded.\n",
      "306,000 sentences embedded.\n",
      "307,000 sentences embedded.\n",
      "308,000 sentences embedded.\n",
      "309,000 sentences embedded.\n",
      "310,000 sentences embedded.\n",
      "311,000 sentences embedded.\n",
      "312,000 sentences embedded.\n",
      "313,000 sentences embedded.\n",
      "314,000 sentences embedded.\n",
      "315,000 sentences embedded.\n",
      "316,000 sentences embedded.\n",
      "317,000 sentences embedded.\n",
      "318,000 sentences embedded.\n",
      "319,000 sentences embedded.\n",
      "320,000 sentences embedded.\n",
      "321,000 sentences embedded.\n",
      "322,000 sentences embedded.\n",
      "323,000 sentences embedded.\n",
      "324,000 sentences embedded.\n",
      "325,000 sentences embedded.\n",
      "326,000 sentences embedded.\n",
      "327,000 sentences embedded.\n",
      "328,000 sentences embedded.\n",
      "329,000 sentences embedded.\n",
      "330,000 sentences embedded.\n",
      "331,000 sentences embedded.\n",
      "332,000 sentences embedded.\n",
      "333,000 sentences embedded.\n",
      "334,000 sentences embedded.\n",
      "335,000 sentences embedded.\n",
      "336,000 sentences embedded.\n",
      "337,000 sentences embedded.\n",
      "338,000 sentences embedded.\n",
      "339,000 sentences embedded.\n",
      "340,000 sentences embedded.\n",
      "341,000 sentences embedded.\n",
      "342,000 sentences embedded.\n",
      "343,000 sentences embedded.\n",
      "344,000 sentences embedded.\n",
      "345,000 sentences embedded.\n",
      "346,000 sentences embedded.\n",
      "347,000 sentences embedded.\n",
      "348,000 sentences embedded.\n",
      "349,000 sentences embedded.\n",
      "350,000 sentences embedded.\n",
      "351,000 sentences embedded.\n",
      "352,000 sentences embedded.\n",
      "353,000 sentences embedded.\n",
      "354,000 sentences embedded.\n",
      "355,000 sentences embedded.\n",
      "356,000 sentences embedded.\n",
      "357,000 sentences embedded.\n",
      "358,000 sentences embedded.\n",
      "359,000 sentences embedded.\n",
      "360,000 sentences embedded.\n",
      "361,000 sentences embedded.\n",
      "362,000 sentences embedded.\n",
      "363,000 sentences embedded.\n",
      "364,000 sentences embedded.\n",
      "365,000 sentences embedded.\n",
      "366,000 sentences embedded.\n",
      "367,000 sentences embedded.\n",
      "368,000 sentences embedded.\n",
      "369,000 sentences embedded.\n",
      "370,000 sentences embedded.\n",
      "371,000 sentences embedded.\n",
      "372,000 sentences embedded.\n",
      "373,000 sentences embedded.\n",
      "374,000 sentences embedded.\n",
      "375,000 sentences embedded.\n",
      "376,000 sentences embedded.\n",
      "377,000 sentences embedded.\n",
      "378,000 sentences embedded.\n",
      "379,000 sentences embedded.\n",
      "380,000 sentences embedded.\n",
      "381,000 sentences embedded.\n",
      "382,000 sentences embedded.\n",
      "383,000 sentences embedded.\n",
      "384,000 sentences embedded.\n",
      "385,000 sentences embedded.\n",
      "386,000 sentences embedded.\n",
      "387,000 sentences embedded.\n",
      "388,000 sentences embedded.\n",
      "389,000 sentences embedded.\n",
      "390,000 sentences embedded.\n",
      "391,000 sentences embedded.\n",
      "392,000 sentences embedded.\n",
      "393,000 sentences embedded.\n",
      "394,000 sentences embedded.\n",
      "395,000 sentences embedded.\n",
      "396,000 sentences embedded.\n",
      "397,000 sentences embedded.\n",
      "398,000 sentences embedded.\n",
      "399,000 sentences embedded.\n",
      "400,000 sentences embedded.\n",
      "401,000 sentences embedded.\n",
      "402,000 sentences embedded.\n",
      "403,000 sentences embedded.\n",
      "404,000 sentences embedded.\n"
     ]
    }
   ],
   "source": [
    "# from util import make_w2v_embeddings\n",
    "# from util import split_and_zero_padding\n",
    "# from util import ManDist\n",
    "\n",
    "# File paths\n",
    "TRAIN_CSV = 'data/train.csv'\n",
    "\n",
    "# Load training set\n",
    "train_df = pd.read_csv(TRAIN_CSV, low_memory=False)\n",
    "for q in ['question1', 'question2']:\n",
    "    train_df[q + '_n'] = train_df[q]\n",
    "\n",
    "# Make word2vec embeddings\n",
    "embedding_dim = 300\n",
    "max_seq_length = 20\n",
    "use_w2v = True\n",
    "\n",
    "train_df, embeddings = make_w2v_embeddings(train_df, embedding_dim=embedding_dim, empty_w2v=not use_w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split to train validation\n",
    "validation_size = int(len(train_df) * 0.1)\n",
    "training_size = len(train_df) - validation_size\n",
    "\n",
    "X = train_df[['question1_n', 'question2_n']]\n",
    "Y = train_df['is_duplicate']\n",
    "\n",
    "X_train, X_validation, Y_train, Y_validation = train_test_split(X, Y, test_size=validation_size)\n",
    "\n",
    "X_train = split_and_zero_padding(X_train, max_seq_length)\n",
    "X_validation = split_and_zero_padding(X_validation, max_seq_length)\n",
    "\n",
    "# Convert labels to their numpy representations\n",
    "Y_train = Y_train.values\n",
    "Y_validation = Y_validation.values\n",
    "\n",
    "# Make sure everything is ok\n",
    "assert X_train['left'].shape == X_train['right'].shape\n",
    "assert len(X_train['left']) == len(Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 20)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 20)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "sequential (Sequential)         (None, 50)           25827000    input_1[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "man_dist (ManDist)              (None, 1)            0           sequential[1][0]                 \n",
      "                                                                 sequential[2][0]                 \n",
      "==================================================================================================\n",
      "Total params: 25,827,000\n",
      "Trainable params: 70,200\n",
      "Non-trainable params: 25,756,800\n",
      "__________________________________________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 20, 300)           25756800  \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 50)                70200     \n",
      "=================================================================\n",
      "Total params: 25,827,000\n",
      "Trainable params: 70,200\n",
      "Non-trainable params: 25,756,800\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Model variables\n",
    "gpus = 1\n",
    "batch_size = 1024 * gpus\n",
    "n_epoch = 50\n",
    "n_hidden = 50\n",
    "\n",
    "# Define the shared model\n",
    "x = Sequential()\n",
    "x.add(Embedding(len(embeddings), embedding_dim,\n",
    "                weights=[embeddings], input_shape=(max_seq_length,), trainable=False))\n",
    "# CNN\n",
    "# x.add(Conv1D(250, kernel_size=5, activation='relu'))\n",
    "# x.add(GlobalMaxPool1D())\n",
    "# x.add(Dense(250, activation='relu'))\n",
    "# x.add(Dropout(0.3))\n",
    "# x.add(Dense(1, activation='sigmoid'))\n",
    "# LSTM\n",
    "x.add(LSTM(n_hidden))\n",
    "\n",
    "shared_model = x\n",
    "\n",
    "# The visible layer\n",
    "left_input = Input(shape=(max_seq_length,), dtype='int32')\n",
    "right_input = Input(shape=(max_seq_length,), dtype='int32')\n",
    "\n",
    "# Pack it all up into a Manhattan Distance model\n",
    "malstm_distance = ManDist()([shared_model(left_input), shared_model(right_input)])\n",
    "model = Model(inputs=[left_input, right_input], outputs=[malstm_distance])\n",
    "\n",
    "# if gpus >= 2:\n",
    "#     # `multi_gpu_model()` is a so quite buggy. it breaks the saved model.\n",
    "#      model = tf.keras.utils.multi_gpu_model(model, gpus=gpus)\n",
    "model.compile(loss='mean_squared_error', optimizer=tf.keras.optimizers.Adam(), metrics=['accuracy'])\n",
    "model.summary()\n",
    "shared_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 363861 samples, validate on 40429 samples\n",
      "Epoch 1/50\n",
      "363861/363861 [==============================] - 64s 175us/step - loss: 0.1825 - acc: 0.7379 - val_loss: 0.1687 - val_acc: 0.7639\n",
      "Epoch 2/50\n",
      "363861/363861 [==============================] - 61s 167us/step - loss: 0.1612 - acc: 0.7760 - val_loss: 0.1599 - val_acc: 0.7782\n",
      "Epoch 3/50\n",
      "363861/363861 [==============================] - 61s 167us/step - loss: 0.1535 - acc: 0.7897 - val_loss: 0.1543 - val_acc: 0.7849\n",
      "Epoch 4/50\n",
      "363861/363861 [==============================] - 61s 166us/step - loss: 0.1486 - acc: 0.7980 - val_loss: 0.1506 - val_acc: 0.7933\n",
      "Epoch 5/50\n",
      "363861/363861 [==============================] - 61s 166us/step - loss: 0.1451 - acc: 0.8036 - val_loss: 0.1486 - val_acc: 0.7975\n",
      "Epoch 6/50\n",
      "363861/363861 [==============================] - 61s 166us/step - loss: 0.1424 - acc: 0.8082 - val_loss: 0.1461 - val_acc: 0.8007\n",
      "Epoch 7/50\n",
      "363861/363861 [==============================] - 61s 167us/step - loss: 0.1402 - acc: 0.8116 - val_loss: 0.1448 - val_acc: 0.8006\n",
      "Epoch 8/50\n",
      "363861/363861 [==============================] - 61s 167us/step - loss: 0.1382 - acc: 0.8149 - val_loss: 0.1432 - val_acc: 0.8059\n",
      "Epoch 9/50\n",
      "363861/363861 [==============================] - 61s 167us/step - loss: 0.1365 - acc: 0.8173 - val_loss: 0.1423 - val_acc: 0.8076\n",
      "Epoch 10/50\n",
      "363861/363861 [==============================] - 61s 167us/step - loss: 0.1350 - acc: 0.8197 - val_loss: 0.1414 - val_acc: 0.8103\n",
      "Epoch 11/50\n",
      "363861/363861 [==============================] - 61s 167us/step - loss: 0.1337 - acc: 0.8217 - val_loss: 0.1414 - val_acc: 0.8042\n",
      "Epoch 12/50\n",
      "363861/363861 [==============================] - 61s 167us/step - loss: 0.1324 - acc: 0.8238 - val_loss: 0.1395 - val_acc: 0.8111\n",
      "Epoch 13/50\n",
      "363861/363861 [==============================] - 61s 167us/step - loss: 0.1312 - acc: 0.8257 - val_loss: 0.1389 - val_acc: 0.8138\n",
      "Epoch 14/50\n",
      "363861/363861 [==============================] - 61s 167us/step - loss: 0.1302 - acc: 0.8276 - val_loss: 0.1381 - val_acc: 0.8125\n",
      "Epoch 15/50\n",
      "363861/363861 [==============================] - 61s 167us/step - loss: 0.1292 - acc: 0.8288 - val_loss: 0.1376 - val_acc: 0.8155\n",
      "Epoch 16/50\n",
      "363861/363861 [==============================] - 61s 167us/step - loss: 0.1282 - acc: 0.8306 - val_loss: 0.1370 - val_acc: 0.8143\n",
      "Epoch 17/50\n",
      "363861/363861 [==============================] - 61s 167us/step - loss: 0.1273 - acc: 0.8317 - val_loss: 0.1366 - val_acc: 0.8144\n",
      "Epoch 18/50\n",
      "363861/363861 [==============================] - 61s 167us/step - loss: 0.1265 - acc: 0.8332 - val_loss: 0.1369 - val_acc: 0.8175\n",
      "Epoch 19/50\n",
      "363861/363861 [==============================] - 61s 168us/step - loss: 0.1258 - acc: 0.8343 - val_loss: 0.1360 - val_acc: 0.8180\n",
      "Epoch 20/50\n",
      "363861/363861 [==============================] - 61s 167us/step - loss: 0.1251 - acc: 0.8356 - val_loss: 0.1354 - val_acc: 0.8178\n",
      "Epoch 21/50\n",
      "363861/363861 [==============================] - 61s 167us/step - loss: 0.1244 - acc: 0.8368 - val_loss: 0.1351 - val_acc: 0.8178\n",
      "Epoch 22/50\n",
      "363861/363861 [==============================] - 61s 167us/step - loss: 0.1237 - acc: 0.8379 - val_loss: 0.1351 - val_acc: 0.8180\n",
      "Epoch 23/50\n",
      "363861/363861 [==============================] - 61s 167us/step - loss: 0.1231 - acc: 0.8388 - val_loss: 0.1350 - val_acc: 0.8198\n",
      "Epoch 24/50\n",
      "363861/363861 [==============================] - 61s 167us/step - loss: 0.1225 - acc: 0.8399 - val_loss: 0.1342 - val_acc: 0.8201\n",
      "Epoch 25/50\n",
      "363861/363861 [==============================] - 61s 167us/step - loss: 0.1219 - acc: 0.8406 - val_loss: 0.1340 - val_acc: 0.8200\n",
      "Epoch 26/50\n",
      "363861/363861 [==============================] - 63s 172us/step - loss: 0.1213 - acc: 0.8415 - val_loss: 0.1340 - val_acc: 0.8184\n",
      "Epoch 27/50\n",
      "363861/363861 [==============================] - 61s 168us/step - loss: 0.1208 - acc: 0.8427 - val_loss: 0.1337 - val_acc: 0.8197\n",
      "Epoch 28/50\n",
      "363861/363861 [==============================] - 61s 167us/step - loss: 0.1203 - acc: 0.8433 - val_loss: 0.1338 - val_acc: 0.8205\n",
      "Epoch 29/50\n",
      "363861/363861 [==============================] - 61s 167us/step - loss: 0.1198 - acc: 0.8439 - val_loss: 0.1331 - val_acc: 0.8219\n",
      "Epoch 30/50\n",
      "363861/363861 [==============================] - 61s 167us/step - loss: 0.1193 - acc: 0.8453 - val_loss: 0.1334 - val_acc: 0.8210\n",
      "Epoch 31/50\n",
      "363861/363861 [==============================] - 61s 167us/step - loss: 0.1187 - acc: 0.8460 - val_loss: 0.1330 - val_acc: 0.8220\n",
      "Epoch 32/50\n",
      "363861/363861 [==============================] - 61s 167us/step - loss: 0.1183 - acc: 0.8468 - val_loss: 0.1325 - val_acc: 0.8233\n",
      "Epoch 33/50\n",
      "363861/363861 [==============================] - 61s 167us/step - loss: 0.1178 - acc: 0.8476 - val_loss: 0.1328 - val_acc: 0.8211\n",
      "Epoch 34/50\n",
      "363861/363861 [==============================] - 61s 167us/step - loss: 0.1174 - acc: 0.8480 - val_loss: 0.1325 - val_acc: 0.8229\n",
      "Epoch 35/50\n",
      "363861/363861 [==============================] - 61s 167us/step - loss: 0.1170 - acc: 0.8486 - val_loss: 0.1324 - val_acc: 0.8237\n",
      "Epoch 36/50\n",
      "363861/363861 [==============================] - 61s 167us/step - loss: 0.1166 - acc: 0.8495 - val_loss: 0.1324 - val_acc: 0.8217\n",
      "Epoch 37/50\n",
      "363861/363861 [==============================] - 61s 167us/step - loss: 0.1162 - acc: 0.8502 - val_loss: 0.1327 - val_acc: 0.8215\n",
      "Epoch 38/50\n",
      "363861/363861 [==============================] - 61s 167us/step - loss: 0.1158 - acc: 0.8507 - val_loss: 0.1322 - val_acc: 0.8240\n",
      "Epoch 39/50\n",
      "363861/363861 [==============================] - 61s 167us/step - loss: 0.1154 - acc: 0.8515 - val_loss: 0.1325 - val_acc: 0.8235\n",
      "Epoch 40/50\n",
      "363861/363861 [==============================] - 61s 167us/step - loss: 0.1149 - acc: 0.8519 - val_loss: 0.1322 - val_acc: 0.8231\n",
      "Epoch 41/50\n",
      "363861/363861 [==============================] - 61s 167us/step - loss: 0.1146 - acc: 0.8529 - val_loss: 0.1318 - val_acc: 0.8242\n",
      "Epoch 42/50\n",
      "363861/363861 [==============================] - 61s 167us/step - loss: 0.1143 - acc: 0.8532 - val_loss: 0.1320 - val_acc: 0.8228\n",
      "Epoch 43/50\n",
      "363861/363861 [==============================] - 61s 167us/step - loss: 0.1139 - acc: 0.8538 - val_loss: 0.1322 - val_acc: 0.8222\n",
      "Epoch 44/50\n",
      "363861/363861 [==============================] - 61s 167us/step - loss: 0.1134 - acc: 0.8548 - val_loss: 0.1321 - val_acc: 0.8248\n",
      "Epoch 45/50\n",
      "363861/363861 [==============================] - 61s 167us/step - loss: 0.1132 - acc: 0.8553 - val_loss: 0.1318 - val_acc: 0.8253\n",
      "Epoch 46/50\n",
      "363861/363861 [==============================] - 61s 167us/step - loss: 0.1129 - acc: 0.8553 - val_loss: 0.1318 - val_acc: 0.8246\n",
      "Epoch 47/50\n",
      "363861/363861 [==============================] - 61s 167us/step - loss: 0.1126 - acc: 0.8560 - val_loss: 0.1316 - val_acc: 0.8253\n",
      "Epoch 48/50\n",
      "363861/363861 [==============================] - 61s 168us/step - loss: 0.1122 - acc: 0.8567 - val_loss: 0.1314 - val_acc: 0.8252\n",
      "Epoch 49/50\n",
      "363861/363861 [==============================] - 61s 167us/step - loss: 0.1119 - acc: 0.8570 - val_loss: 0.1313 - val_acc: 0.8265\n",
      "Epoch 50/50\n",
      "363861/363861 [==============================] - 61s 167us/step - loss: 0.1116 - acc: 0.8576 - val_loss: 0.1315 - val_acc: 0.8255\n",
      "Training time finished.\n",
      "50 epochs in      3043.22\n"
     ]
    }
   ],
   "source": [
    "# Start trainings\n",
    "training_start_time = time()\n",
    "malstm_trained = model.fit([X_train['left'], X_train['right']], Y_train,\n",
    "                           batch_size=batch_size, epochs=n_epoch,\n",
    "                           validation_data=([X_validation['left'], X_validation['right']], Y_validation))\n",
    "training_end_time = time()\n",
    "print(\"Training time finished.\\n%d epochs in %12.2f\" % (n_epoch,\n",
    "                                                        training_end_time - training_start_time))\n",
    "\n",
    "model.save('SiameseLSTM.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8254(max: 0.8264)\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# Plot accuracy\n",
    "plt.subplot(211)\n",
    "plt.plot(malstm_trained.history['acc'])\n",
    "plt.plot(malstm_trained.history['val_acc'])\n",
    "plt.title('Model Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "\n",
    "# Plot loss\n",
    "plt.subplot(212)\n",
    "plt.plot(malstm_trained.history['loss'])\n",
    "plt.plot(malstm_trained.history['val_loss'])\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper right')\n",
    "\n",
    "plt.tight_layout(h_pad=1.0)\n",
    "plt.savefig('history-graph.png')\n",
    "\n",
    "print(str(malstm_trained.history['val_acc'][-1])[:6] +\n",
    "      \"(max: \" + str(max(malstm_trained.history['val_acc']))[:6] + \")\")\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'session' in locals() and session is not None:\n",
    "    print('Close interactive session')\n",
    "    session.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
